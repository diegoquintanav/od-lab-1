{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.7.4 (default, Oct 15 2019, 22:29:14) \\n[GCC 7.4.0]'"
      ]
     },
     "execution_count": 389,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7.6\n",
      "4.3.0\n"
     ]
    }
   ],
   "source": [
    "import neo4j\n",
    "import py2neo\n",
    "print(neo4j.__version__)\n",
    "print(py2neo.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "\n",
    "# instantiate driver\n",
    "NEO4J_URI=\"bolt://localhost:7687\"\n",
    "gdb = GraphDatabase.driver(uri=NEO4J_URI, auth=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A.2 Loading Data\n",
    "\n",
    "\n",
    "We will read data from dblp.uni-trier.de. From the XML's description of data in https://dblp.org/faq/16154937.html, the following elements are represented\n",
    "\n",
    "> - article – An article from a journal or magazine.\n",
    "> - inproceedings – A paper in a conference or workshop proceedings.\n",
    "> - proceedings – The proceedings volume of a conference or workshop.\n",
    "> - book – An authored monograph or an edited collection of articles.\n",
    "> - incollection – A part or chapter in a monograph.\n",
    "> - phdthesis – A PhD thesis.\n",
    "> - mastersthesis – A Master's thesis. There are only very few Master's theses in dblp.\n",
    "> - www – A web page. There are only very few web pages in dblp. See also the notes on person records.\n",
    "\n",
    "We will rely on the script provided in https://github.com/ThomHurks/dblp-to-csv, and we will be removing some of the elements by editing from the `dtd` file. In particular we will be removing\n",
    "\n",
    "- book\n",
    "- incollection\n",
    "- phdthesis\n",
    "- masterthesis\n",
    "- www\n",
    "\n",
    "The script is then executed as\n",
    "\n",
    "```bash\n",
    "#!/bin/bash\n",
    "./XMLToCSV.py --annotate --neo4j dblp-raw/dblp.xml dblp-raw/dblp_slim.dtd output_slim/output.csv --relations author:authored_by journal:published_in publisher:published_by school:submitted_at editor:edited_by cite:has_citation\n",
    "```\n",
    "\n",
    "and the `neo4j-admin import` command is\n",
    "\n",
    "```bash\n",
    "#!/bin/bash\n",
    "neo4j-admin import --mode=csv --database=dblp_slim.db --delimiter \";\" --array-delimiter \"|\" --id-type INTEGER --nodes:inproceedings \"output_slim/output_inproceedings_header.csv,output_slim/output_inproceedings.csv\" --nodes:article \"output_slim/output_article_header.csv,output_slim/output_article.csv\" --nodes:proceedings \"output_slim/output_proceedings_header.csv,output_slim/output_proceedings.csv\" --nodes:editor \"output_slim/output_editor.csv\" --relationships:edited_by \"output_slim/output_editor_edited_by.csv\" --nodes:publisher \"output_slim/output_publisher.csv\" --relationships:published_by \"output_slim/output_publisher_published_by.csv\" --nodes:journal \"output_slim/output_journal.csv\" --relationships:published_in \"output_slim/output_journal_published_in.csv\" --nodes:author \"output_slim/output_author.csv\" --relationships:authored_by \"output_slim/output_author_authored_by.csv\" --nodes:cite \"output_slim/output_cite.csv\" --relationships:has_citation \"output_slim/output_cite_has_citation.csv\"\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "By modifying the `dtd` file, we obtain a smaller graph\n",
    "- node count from 9,985,270 to 7,338, 701\n",
    "- relationship count from 19,917,751 to 17,079,387\n",
    "\n",
    "Finally, running the scripts, we get something like this\n",
    "\n",
    "![schema1](images/graph.png)\n",
    "\n",
    "## Missing nodes and relationships\n",
    "\n",
    "We are then missing the following nodes\n",
    "\n",
    "- topics\n",
    "- keywords\n",
    "- journals\n",
    "- volumes\n",
    "\n",
    "and the following relationships\n",
    "\n",
    "- topic -> has -> keywords\n",
    "- article -> cited_by -> article\n",
    "- author -> reviews -> article\n",
    "\n",
    "## Faking citations\n",
    "\n",
    "citations are hard to parse from xml data, so we will be randomly linking articles between them using the `cited_in` relationship\n",
    "\n",
    "creating a relationship\n",
    "\n",
    "```cypher\n",
    "MATCH (a:article),(b:article)\n",
    "WHERE ID(a) = 12 AND ID(b) = 13\n",
    "CREATE (a)-[r:cited_by]->(b)\n",
    "RETURN type(r)\n",
    "```\n",
    "\n",
    "deleting a relationship\n",
    "\n",
    "```cypher\n",
    "MATCH p=(:article)-[r:cited_by]->(:article) delete r\n",
    "```\n",
    "\n",
    "query relationships\n",
    "\n",
    "```\n",
    "MATCH p=(:article)-[r:cited_by]->(:article) RETURN p LIMIT 25\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fake citations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetch existing articles IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2508965, 2508966, 2508967, 2508968, 2508969]"
      ]
     },
     "execution_count": 410,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = \"MATCH (n:article) RETURN ID(n) LIMIT 10000\"\n",
    "\n",
    "with gdb.session() as session:\n",
    "    article_ids = [v[0] for v in session.run(q).values()]\n",
    "    \n",
    "article_ids[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optional, add proceedings and journals as citable elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 406,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(article_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Delete citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete existing citations before inserting new ones\n",
    "with gdb.session() as session:\n",
    "    session.run(\"MATCH p=(:article)-[r:cited_by]->(:article) DELETE r \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create `cited_by` relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://neo4j.com/docs/driver-manual/1.7/sessions-transactions/#driver-transactions-transaction-functions\n",
    "\n",
    "q_add_citation_rel_id = \"\"\"MATCH (a:article),(b:article)\n",
    "WHERE ID(a) = $id_a AND ID(b) = $id_b\n",
    "CREATE (a)-[r:cited_by]->(b)\n",
    "RETURN a, b\"\"\"\n",
    "\n",
    "\n",
    "def add_citation_rel(driver, id_a, id_b):\n",
    "    with driver.session() as session:\n",
    "        # Caller for transactional unit of work\n",
    "        return session.write_transaction(create_citation_rel, id_a, id_b)\n",
    "\n",
    "# Simple implementation of the unit of work\n",
    "def create_citation_rel(tx, id_a, id_b):\n",
    "    return tx.run(q_add_citation_rel_id, id_a=id_a, id_b = id_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add 500 relationships of type `cited_by`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2542972 2542972\n"
     ]
    }
   ],
   "source": [
    "random.seed(42)\n",
    "# pick a sample of 500 papers, and make them be cited by other three papers at random\n",
    "created = []\n",
    "\n",
    "for article in random.sample(article_ids, 500):\n",
    "    for citation in random.sample(article_ids, 3):\n",
    "        if article != citation: # they can't cite themselves\n",
    "            created.append((article, citation))\n",
    "            add_citation_rel(gdb, id_a=article, id_b=citation)\n",
    "        else:\n",
    "            print(article, citation)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2527960, 2560941),\n",
       " (2527960, 2536901),\n",
       " (2527960, 2552365),\n",
       " (2509374, 2556357),\n",
       " (2509374, 2553167)]"
      ]
     },
     "execution_count": 417,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "created[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make articles `UNIQUE`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no repeated articles\n",
    "with gdb.session() as session:\n",
    "    session.run(\"CREATE CONSTRAINT ON (n:article) ASSERT n.article IS UNIQUE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<Relationship id=17081502 nodes=(<Node id=2508971 labels=set() properties={}>, <Node id=2548988 labels=set() properties={}>) type='cited_by' properties={}>]\n",
      "[<Relationship id=17081501 nodes=(<Node id=2508971 labels=set() properties={}>, <Node id=2557826 labels=set() properties={}>) type='cited_by' properties={}>]\n",
      "[<Relationship id=17081500 nodes=(<Node id=2508971 labels=set() properties={}>, <Node id=2555890 labels=set() properties={}>) type='cited_by' properties={}>]\n",
      "[<Relationship id=17080549 nodes=(<Node id=2508974 labels=set() properties={}>, <Node id=2557687 labels=set() properties={}>) type='cited_by' properties={}>]\n",
      "[<Relationship id=17080548 nodes=(<Node id=2508974 labels=set() properties={}>, <Node id=2545595 labels=set() properties={}>) type='cited_by' properties={}>]\n"
     ]
    }
   ],
   "source": [
    "with gdb.session() as session:\n",
    "    out = session.run(\"MATCH p=(:article)-[r:cited_by]->(:article) RETURN r LIMIT 5\").values()\n",
    "    for elem in out:\n",
    "        print(elem)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fake keywords\n",
    "It's not trivial to parse keywords and topics from data, so we will fake some topics and random keywords using the `faker` library, as explained in http://www.jexp.de/blog/html/create_random_data.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "from faker import Faker\n",
    "from faker.providers import lorem\n",
    "\n",
    "fake = Faker()\n",
    "fake.seed_instance(42)\n",
    "fake.add_provider(lorem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 420,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# +-100 fake, non repeated keywords\n",
    "fake_keywords = [(ix, word) for ix, word in enumerate(list({fake.sentence(nb_words=3) for _ in range(100)}), start=1)]\n",
    "len(fake_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 'Behavior benefit.'),\n",
       " (2, 'Nation strong.'),\n",
       " (3, 'Campaign little.'),\n",
       " (4, 'Last everything.'),\n",
       " (5, 'Better present music.'),\n",
       " (6, 'Water beat magazine.'),\n",
       " (7, 'War real.'),\n",
       " (8, 'Lead upon.'),\n",
       " (9, 'Article finish anyone.'),\n",
       " (10, 'Page southern role.')]"
      ]
     },
     "execution_count": 421,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_keywords[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete keywords\n",
    "with gdb.session() as session:\n",
    "    for ix, keyword in fake_keywords:\n",
    "        session.run(\"MATCH (n:keyword) DELETE n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_create_keyword = \"CREATE (:keyword {id:$id, keyword:$keyword})\"\n",
    "\n",
    "with gdb.session() as session:\n",
    "    for ix, keyword in fake_keywords:\n",
    "        session.run(q_create_keyword, id=ix, keyword=keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create UNIQUE constraint\n",
    "with gdb.session() as session:\n",
    "    session.run(\"CREATE CONSTRAINT ON (n:keyword) ASSERT n.keyword IS UNIQUE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assign 5 keywords to 1000 articles, at random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete relationships\n",
    "with gdb.session() as session:\n",
    "    session.run(\"MATCH p=(:article)-[r:has_keyword]->(:keyword) delete r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign 5 keywords randomly to 1000 articles\n",
    "random.seed(42)\n",
    "\n",
    "q_add_keywords = \"\"\"MATCH (a:article),(b:keyword)\n",
    "WHERE ID(a) = $article_id AND b.keyword = $keyword\n",
    "CREATE (a)-[r:has_keyword]->(b)\n",
    "RETURN a, b\"\"\"\n",
    "\n",
    "created_kw_rel = []\n",
    "\n",
    "with gdb.session() as session:\n",
    "    for article_id in random.sample(article_ids, 1000):\n",
    "        for ix, keyword in random.sample(fake_keywords, 5):\n",
    "            created_kw_rel.append((article_id, keyword))\n",
    "            session.run(q_add_keywords, article_id=article_id, keyword=keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2527960, 'Necessary into act.'),\n",
       " (2527960, 'You available defense.'),\n",
       " (2527960, 'Prove reduce.'),\n",
       " (2527960, 'Coach magazine.'),\n",
       " (2527960, 'Economy traditional anything.')]"
      ]
     },
     "execution_count": 441,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "created_kw_rel[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gdb.session() as session:\n",
    "    out = session.run(\"MATCH p=(:article)-[r:has_keyword]->(:keyword) RETURN r LIMIT 5\").values()\n",
    "    for elem in out:\n",
    "        print(elem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A.3 Evolving the graph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
