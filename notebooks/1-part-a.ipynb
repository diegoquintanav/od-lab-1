{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.7.4 (default, Oct 15 2019, 22:29:14) \\n[GCC 7.4.0]'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7.6\n",
      "4.3.0\n"
     ]
    }
   ],
   "source": [
    "import neo4j\n",
    "import py2neo\n",
    "print(neo4j.__version__)\n",
    "print(py2neo.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "\n",
    "# instantiate driver\n",
    "NEO4J_URI=\"bolt://localhost:7687\"\n",
    "gdb = GraphDatabase.driver(uri=NEO4J_URI, auth=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A.2 Loading Data\n",
    "\n",
    "\n",
    "We will read data from dblp.uni-trier.de. From the XML's description of data in https://dblp.org/faq/16154937.html, the following elements are represented\n",
    "\n",
    "> - article – An article from a journal or magazine.\n",
    "> - inproceedings – A paper in a conference or workshop proceedings.\n",
    "> - proceedings – The proceedings volume of a conference or workshop.\n",
    "> - book – An authored monograph or an edited collection of articles.\n",
    "> - incollection – A part or chapter in a monograph.\n",
    "> - phdthesis – A PhD thesis.\n",
    "> - mastersthesis – A Master's thesis. There are only very few Master's theses in dblp.\n",
    "> - www – A web page. There are only very few web pages in dblp. See also the notes on person records.\n",
    "\n",
    "We will rely on the script provided in https://github.com/ThomHurks/dblp-to-csv, and we will be removing some of the elements by editing from the `dtd` file. In particular we will be removing\n",
    "\n",
    "- book\n",
    "- incollection\n",
    "- phdthesis\n",
    "- masterthesis\n",
    "- www\n",
    "\n",
    "The script is then executed as\n",
    "\n",
    "```bash\n",
    "#!/bin/bash\n",
    "./XMLToCSV.py --annotate --neo4j dblp-raw/dblp.xml dblp-raw/dblp_slim.dtd output_slim/output.csv --relations author:authored_by journal:published_in publisher:published_by school:submitted_at editor:edited_by cite:has_citation\n",
    "```\n",
    "\n",
    "and the `neo4j-admin import` command is\n",
    "\n",
    "```bash\n",
    "#!/bin/bash\n",
    "neo4j-admin import --mode=csv --database=dblp_slim.db --delimiter \";\" --array-delimiter \"|\" --id-type INTEGER --nodes:inproceedings \"output_slim/output_inproceedings_header.csv,output_slim/output_inproceedings.csv\" --nodes:article \"output_slim/output_article_header.csv,output_slim/output_article.csv\" --nodes:proceedings \"output_slim/output_proceedings_header.csv,output_slim/output_proceedings.csv\" --nodes:editor \"output_slim/output_editor.csv\" --relationships:edited_by \"output_slim/output_editor_edited_by.csv\" --nodes:publisher \"output_slim/output_publisher.csv\" --relationships:published_by \"output_slim/output_publisher_published_by.csv\" --nodes:journal \"output_slim/output_journal.csv\" --relationships:published_in \"output_slim/output_journal_published_in.csv\" --nodes:author \"output_slim/output_author.csv\" --relationships:authored_by \"output_slim/output_author_authored_by.csv\" --nodes:cite \"output_slim/output_cite.csv\" --relationships:has_citation \"output_slim/output_cite_has_citation.csv\"\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "By modifying the `dtd` file, we obtain a smaller graph\n",
    "- node count from 9,985,270 to 7,338, 701\n",
    "- relationship count from 19,917,751 to 17,079,387\n",
    "\n",
    "Finally, running the scripts, we get something like this\n",
    "\n",
    "![schema1](images/graph.png)\n",
    "\n",
    "## Missing nodes and relationships\n",
    "\n",
    "We are then missing the following nodes\n",
    "\n",
    "- topics\n",
    "- keywords\n",
    "- journals\n",
    "- volumes\n",
    "\n",
    "and the following relationships\n",
    "\n",
    "- topic -> has -> keywords\n",
    "- article -> cited_by -> article\n",
    "- author -> reviews -> article\n",
    "\n",
    "## Faking citations\n",
    "\n",
    "citations are hard to parse from xml data, so we will be randomly linking articles between them using the `cited_in` relationship\n",
    "\n",
    "creating a relationship\n",
    "\n",
    "```cypher\n",
    "MATCH (a:article),(b:article)\n",
    "WHERE ID(a) = 12 AND ID(b) = 13\n",
    "CREATE (a)-[r:cited_by]->(b)\n",
    "RETURN type(r)\n",
    "```\n",
    "\n",
    "deleting a relationship\n",
    "\n",
    "```cypher\n",
    "MATCH p=(:article)-[r:cited_by]->(:article) delete r\n",
    "```\n",
    "\n",
    "query relationships\n",
    "\n",
    "```\n",
    "MATCH p=(:article)-[r:cited_by]->(:article) RETURN p LIMIT 25\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fake citations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetch existing articles IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2508965, 2508966, 2508967, 2508968, 2508969]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = \"MATCH (n:article) RETURN ID(n) LIMIT 10000\"\n",
    "\n",
    "with gdb.session() as session:\n",
    "    article_ids = [v[0] for v in session.run(q).values()]\n",
    "    \n",
    "article_ids[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optional, add proceedings and journals as citable elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(article_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Delete citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete existing citations before inserting new ones\n",
    "with gdb.session() as session:\n",
    "    session.run(\"MATCH p=(:article)-[r:cited_by]->(:article) DELETE r \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create `cited_by` relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://neo4j.com/docs/driver-manual/1.7/sessions-transactions/#driver-transactions-transaction-functions\n",
    "\n",
    "q_add_citation_rel_id = \"\"\"MATCH (a:article),(b:article)\n",
    "WHERE ID(a) = $id_a AND ID(b) = $id_b\n",
    "CREATE (a)-[r:cited_by]->(b)\n",
    "RETURN a, b\"\"\"\n",
    "\n",
    "\n",
    "def add_citation_rel(driver, id_a, id_b):\n",
    "    with driver.session() as session:\n",
    "        # Caller for transactional unit of work\n",
    "        return session.write_transaction(create_citation_rel, id_a, id_b)\n",
    "\n",
    "# Simple implementation of the unit of work\n",
    "def create_citation_rel(tx, id_a, id_b):\n",
    "    return tx.run(q_add_citation_rel_id, id_a=id_a, id_b = id_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add 500 relationships of type `cited_by`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2542972 2542972\n"
     ]
    }
   ],
   "source": [
    "random.seed(42)\n",
    "# pick a sample of 500 papers, and make them be cited by other three papers at random\n",
    "created = []\n",
    "\n",
    "for article in random.sample(article_ids, 500):\n",
    "    for citation in random.sample(article_ids, 3):\n",
    "        if article != citation: # they can't cite themselves\n",
    "            created.append((article, citation))\n",
    "            add_citation_rel(gdb, id_a=article, id_b=citation)\n",
    "        else:\n",
    "            print(article, citation)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2527960, 2560941),\n",
       " (2527960, 2536901),\n",
       " (2527960, 2552365),\n",
       " (2509374, 2556357),\n",
       " (2509374, 2553167)]"
      ]
     },
     "execution_count": 580,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "created[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make articles `UNIQUE`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no repeated articles\n",
    "with gdb.session() as session:\n",
    "    session.run(\"CREATE CONSTRAINT ON (n:article) ASSERT n.article IS UNIQUE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<Relationship id=17082515 nodes=(<Node id=2508971 labels=set() properties={}>, <Node id=2548988 labels=set() properties={}>) type='cited_by' properties={}>]\n",
      "[<Relationship id=17082514 nodes=(<Node id=2508971 labels=set() properties={}>, <Node id=2557826 labels=set() properties={}>) type='cited_by' properties={}>]\n",
      "[<Relationship id=17082513 nodes=(<Node id=2508971 labels=set() properties={}>, <Node id=2555890 labels=set() properties={}>) type='cited_by' properties={}>]\n",
      "[<Relationship id=17081563 nodes=(<Node id=2508974 labels=set() properties={}>, <Node id=2557687 labels=set() properties={}>) type='cited_by' properties={}>]\n",
      "[<Relationship id=17081562 nodes=(<Node id=2508974 labels=set() properties={}>, <Node id=2545595 labels=set() properties={}>) type='cited_by' properties={}>]\n"
     ]
    }
   ],
   "source": [
    "with gdb.session() as session:\n",
    "    out = session.run(\"MATCH p=(:article)-[r:cited_by]->(:article) RETURN r LIMIT 5\").values()\n",
    "    for elem in out:\n",
    "        print(elem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fake keywords\n",
    "It's not trivial to parse keywords and topics from data, so we will fake some topics and random keywords using the `faker` library, as explained in http://www.jexp.de/blog/html/create_random_data.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from faker import Faker\n",
    "from faker.providers import lorem\n",
    "\n",
    "fake = Faker()\n",
    "fake.seed_instance(42)\n",
    "fake.add_provider(lorem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# +-100 fake, non repeated keywords\n",
    "fake_keywords = [(ix, word) for ix, word in enumerate(list({fake.sentence(nb_words=3).rstrip('.') for _ in range(100)}), start=1)]\n",
    "len(fake_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 'Account stage federal'),\n",
       " (2, 'Stop peace'),\n",
       " (3, 'Behavior benefit'),\n",
       " (4, 'Tree that fear'),\n",
       " (5, 'Term herself'),\n",
       " (6, 'East organization people'),\n",
       " (7, 'Why'),\n",
       " (8, 'Bag control organization'),\n",
       " (9, 'Interest level rock'),\n",
       " (10, 'Discover detail audience')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_keywords[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete keywords\n",
    "with gdb.session() as session:\n",
    "    session.run(\"MATCH (n:keyword) DELETE n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_create_keyword = \"CREATE (:keyword {id:$id, keyword:$keyword})\"\n",
    "\n",
    "with gdb.session() as session:\n",
    "    for ix, keyword in fake_keywords:\n",
    "        session.run(q_create_keyword, id=ix, keyword=keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create UNIQUE constraint\n",
    "with gdb.session() as session:\n",
    "    session.run(\"CREATE CONSTRAINT ON (n:keyword) ASSERT n.keyword IS UNIQUE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assign 5 keywords to 1000 articles, at random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete relationships\n",
    "with gdb.session() as session:\n",
    "    session.run(\"MATCH p=(:article)-[r:has_keyword]->(:keyword) DELETE r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign 5 keywords randomly to 1000 articles\n",
    "random.seed(42)\n",
    "\n",
    "q_add_keywords = \"\"\"MATCH (a:article),(b:keyword)\n",
    "WHERE ID(a) = $article_id AND b.keyword = $keyword\n",
    "CREATE (a)-[r:has_keyword]->(b)\n",
    "RETURN a, b\"\"\"\n",
    "\n",
    "created_kw_rel = []\n",
    "\n",
    "with gdb.session() as session:\n",
    "    for article_id in random.sample(article_ids, 1000):\n",
    "        for ix, keyword in random.sample(fake_keywords, 5):\n",
    "            created_kw_rel.append((article_id, keyword))\n",
    "            session.run(q_add_keywords, article_id=article_id, keyword=keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2527960, 'Least'),\n",
       " (2527960, 'Author technology amount'),\n",
       " (2527960, 'Message board mean'),\n",
       " (2527960, 'Spend prove stock'),\n",
       " (2527960, 'Quickly appear piece')]"
      ]
     },
     "execution_count": 588,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "created_kw_rel[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<Relationship id=17081012 nodes=(<Node id=2557967 labels=set() properties={}>, <Node id=7340970 labels=set() properties={}>) type='has_keyword' properties={}>]\n",
      "[<Relationship id=17080942 nodes=(<Node id=2553350 labels=set() properties={}>, <Node id=7340970 labels=set() properties={}>) type='has_keyword' properties={}>]\n",
      "[<Relationship id=17080857 nodes=(<Node id=2556280 labels=set() properties={}>, <Node id=7340970 labels=set() properties={}>) type='has_keyword' properties={}>]\n",
      "[<Relationship id=17080765 nodes=(<Node id=2540985 labels=set() properties={}>, <Node id=7340970 labels=set() properties={}>) type='has_keyword' properties={}>]\n",
      "[<Relationship id=17080704 nodes=(<Node id=2527663 labels=set() properties={}>, <Node id=7340970 labels=set() properties={}>) type='has_keyword' properties={}>]\n"
     ]
    }
   ],
   "source": [
    "with gdb.session() as session:\n",
    "    out = session.run(\"MATCH p=(:article)-[r:has_keyword]->(:keyword) RETURN r LIMIT 5\").values()\n",
    "    for elem in out:\n",
    "        print(elem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Faking reviewers "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a `reviewed_by` relationship between authors and articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete relationships\n",
    "with gdb.session() as session:\n",
    "    session.run(\"MATCH p=(:article)-[r:reviewed_by]->(:author) DELETE r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4771927, 4771928, 4771929, 4771930, 4771931]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = \"MATCH (n:author) RETURN ID(n) LIMIT 10000\"\n",
    "\n",
    "with gdb.session() as session:\n",
    "    author_ids = [v[0] for v in session.run(q).values()]\n",
    "    \n",
    "author_ids[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign between 3 and 4 reviewers randomly to 1000 articles\n",
    "random.seed(42)\n",
    "\n",
    "q_add_reviewers = \"\"\"MATCH (a:article),(b:author)\n",
    "WHERE ID(a) = $article_id AND ID(b) = $author_id\n",
    "CREATE (a)-[r:reviewed_by]->(b)\"\"\"\n",
    "\n",
    "created_review_rel = []\n",
    "\n",
    "with gdb.session() as session:\n",
    "    for article_id in random.sample(article_ids, 1000):\n",
    "        for author_id in random.sample(author_ids, 3):\n",
    "            created_review_rel.append((article_id, author_id))\n",
    "            session.run(q_add_reviewers, article_id=article_id, author_id=author_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'created_review_rel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-153d2950425f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcreated_review_rel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'created_review_rel' is not defined"
     ]
    }
   ],
   "source": [
    "created_review_rel[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A.3 Evolving the graph "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The requirements are\n",
    "\n",
    "#### Store the review and the approval sent by each reviewer\n",
    "\n",
    "Since the queries in part B don't need this information, we can store the `review_decision` and the `review_text` as _edge_ attributes of the relation `REVIEWED_BY`.\n",
    "\n",
    "\n",
    "Alternatively, and in the case these reviews were a requirement in part B, they can be implemented by creating a `Review` node, and linking it to articles, in a four step process.\n",
    "\n",
    "1. Create a `Review` article for each `Article` that has reviewers (i.e. is connected to authors by `reviewed_by` edges). Each `Review` node will have an attribute named `review_contents´.\n",
    "2. Attach review to the article\n",
    "3. Attach reviewers to the review instance\n",
    "4. Optional, delete the `reviewed_by` edges from the step 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gdb.session() as session:\n",
    "    relationships = session.run(\"MATCH p=(:article)-[r:reviewed_by]->(:author) return ID(r)\").values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 717,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lorem Ipsum Donor\n"
     ]
    }
   ],
   "source": [
    "for x in out[0][0].relationships:\n",
    "    print(x[\"review_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 737,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17088004 {'review_text': 'Lorem Ipsum Donor', 'review_accepted': True}\n",
      "17088003 {}\n",
      "17088002 {}\n",
      "17087050 {}\n",
      "17087049 {}\n"
     ]
    }
   ],
   "source": [
    "for x in out[:5]:\n",
    "    for i in x:\n",
    "        for r in i.relationships:\n",
    "            print(r.id, dict(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 738,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gdb.session() as session:\n",
    "    out = session.run(\"MATCH p=(:article)-[r:reviewed_by]->(:author) return ID(r)\")\n",
    "    rel_ids = [r[0] for r in out.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 739,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[17088004, 17088003, 17088002, 17087050, 17087049]"
      ]
     },
     "execution_count": 739,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rel_ids[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 743,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_merge_review_attributes = \"\"\"MATCH p=(:article)-[r:reviewed_by]->(:author)\n",
    "WHERE ID(r) = $rel_id\n",
    "SET r = {review_accepted: $review_accepted, review_text: $review_text}\n",
    "\"\"\"\n",
    "\n",
    "created_review_rel_attr = []\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "with gdb.session() as session:\n",
    "    for rel_id in rel_ids:\n",
    "        created_review_rel_attr.append(rel_id)\n",
    "        session.run(q_merge_review_attributes, \n",
    "                    rel_id=rel_id,\n",
    "                    review_accepted=fake.pybool(), \n",
    "                    review_text=fake.texts(nb_texts=1, max_nb_chars=500)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 745,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[17088004, 17088003, 17088002, 17087050, 17087049]"
      ]
     },
     "execution_count": 745,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "created_review_rel_attr[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Store a reviewing policy for each Journal or Conference\n",
    "\n",
    "This can also be implemented as an `review_policy_min_count` to the `Journal` and `Proceedings` labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gdb.session() as session:\n",
    "    journal_ids = session.run(\"MERGE (n:journal) SET n.review_policy_min_count = 3 return n\").values()\n",
    "    proceeding_ids = session.run(\"MERGE (n:proceedings) SET n.review_policy_min_count = 3 return n\").values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[<Node id=4742771 labels={'journal'} properties={'review_policy_min_count': 3, 'journal': 'meltdownattack.com'}>],\n",
       " [<Node id=4742772 labels={'journal'} properties={'review_policy_min_count': 3, 'journal': 'GTE Laboratories Incorporated'}>],\n",
       " [<Node id=4742773 labels={'journal'} properties={'review_policy_min_count': 3, 'journal': 'University of California at Berkeley'}>],\n",
       " [<Node id=4742774 labels={'journal'} properties={'review_policy_min_count': 3, 'journal': 'ANSI X3H2'}>],\n",
       " [<Node id=4742775 labels={'journal'} properties={'review_policy_min_count': 3, 'journal': 'ANSI X2H2'}>]]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "journal_ids[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[<Node id=4687702 labels={'proceedings'} properties={'ee': ['https://doi.org/10.1007/978-3-642-32940-1'], 'editor': ['Irek Ulidowski', 'Maciej Koutny'], 'year': 2012, 'isbn': ['978-3-642-32939-5'], 'series-href': ['db/series/lncs/index.html'], 'title': 'CONCUR 2012 - Concurrency Theory - 23rd International Conference, CONCUR 2012, Newcastle upon Tyne, UK, September 4-7, 2012. Proceedings', 'url': 'db/conf/concur/concur2012.html', 'review_policy_min_count': 3, 'volume': '7454', 'mdate': neotime.Date(2019, 5, 14), 'series': ['Lecture Notes in Computer Science'], 'publisher': ['Springer'], 'proceedings': 4180529, 'booktitle': 'CONCUR', 'key': 'conf/concur/2012'}>],\n",
       " [<Node id=4687703 labels={'proceedings'} properties={'ee': ['https://doi.org/10.1007/11817949'], 'editor': ['Christel Baier', 'Holger Hermanns'], 'year': 2006, 'isbn': ['3-540-37376-4'], 'series-href': ['db/series/lncs/index.html'], 'title': 'CONCUR 2006 - Concurrency Theory, 17th International Conference, CONCUR 2006, Bonn, Germany, August 27-30, 2006, Proceedings', 'url': 'db/conf/concur/concur2006.html', 'review_policy_min_count': 3, 'volume': '4137', 'mdate': neotime.Date(2019, 5, 14), 'series': ['Lecture Notes in Computer Science'], 'publisher': ['Springer'], 'proceedings': 4180547, 'booktitle': 'CONCUR', 'key': 'conf/concur/2006'}>],\n",
       " [<Node id=4687704 labels={'proceedings'} properties={'ee': ['https://doi.org/10.1007/978-3-540-48654-1'], 'editor': ['Bengt Jonsson', 'Joachim Parrow'], 'year': 1994, 'isbn': ['3-540-58329-7'], 'series-href': ['db/series/lncs/index.html'], 'title': \"CONCUR '94, Concurrency Theory, 5th International Conference, Uppsala, Sweden, August 22-25, 1994, Proceedings\", 'url': 'db/conf/concur/concur1994.html', 'review_policy_min_count': 3, 'volume': '836', 'mdate': neotime.Date(2019, 5, 14), 'series': ['Lecture Notes in Computer Science'], 'publisher': ['Springer'], 'proceedings': 4180622, 'booktitle': 'CONCUR', 'key': 'conf/concur/1994'}>],\n",
       " [<Node id=4687705 labels={'proceedings'} properties={'ee': ['https://doi.org/10.1007/3-540-48320-9'], 'editor': ['Jos C. M. Baeten', 'Sjouke Mauw'], 'year': 1999, 'isbn': ['3-540-66425-4'], 'series-href': ['db/series/lncs/index.html'], 'title': \"CONCUR '99: Concurrency Theory, 10th International Conference, Eindhoven, The Netherlands, August 24-27, 1999, Proceedings\", 'url': 'db/conf/concur/concur1999.html', 'review_policy_min_count': 3, 'volume': '1664', 'mdate': neotime.Date(2019, 5, 14), 'series': ['Lecture Notes in Computer Science'], 'publisher': ['Springer'], 'proceedings': 4180661, 'booktitle': 'CONCUR', 'key': 'conf/concur/1999'}>],\n",
       " [<Node id=4687706 labels={'proceedings'} properties={'ee': ['http://www.dagstuhl.de/dagpub/978-3-95977-017-0'], 'editor': ['Josée Desharnais', 'Radha Jagadeesan'], 'year': 2016, 'isbn': ['978-3-95977-017-0'], 'series-href': ['db/series/lipics/index.html'], 'title': '27th International Conference on Concurrency Theory, CONCUR 2016, August 23-26, 2016, Québec City, Canada', 'url': 'db/conf/concur/concur2016.html', 'review_policy_min_count': 3, 'volume': '59', 'mdate': neotime.Date(2020, 2, 11), 'series': ['LIPIcs'], 'ee-type': ['oa'], 'publisher': ['Schloss Dagstuhl - Leibniz-Zentrum für Informatik'], 'proceedings': 4180699, 'booktitle': 'CONCUR', 'key': 'conf/concur/2016'}>]]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proceeding_ids[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the affiliation of the author to an organization or a company \n",
    "\n",
    "This can be solved by creating an `affiliation_institution_type` attribute, and also an `affiliation_institution_name`. We could have created these as nodes, but the queries from part B do not refer to this information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_set_affiliations = \"\"\"MATCH (n:author)\n",
    "WHERE ID(n) = $author_id\n",
    "SET n = {affiliation_institution_type: $affiliation_institution_type, affiliation_institution_name: $affiliation_institution_name}\n",
    "RETURN n\n",
    "\"\"\"\n",
    "\n",
    "institution_types = ['University', 'Company', 'NGO']\n",
    "created_affiliations = []\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "with gdb.session() as session:\n",
    "    for author_id in author_ids:\n",
    "        out = session.run(q_set_affiliations, \n",
    "                    author_id=author_id,\n",
    "                    affiliation_institution_name=fake.company(), \n",
    "                    affiliation_institution_type=random.choice(institution_types))\n",
    "        \n",
    "        created_affiliations.append(out.values())\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
